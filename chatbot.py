
from ner.predict import extract_query_entities
from document_retrieval.retriever import search_documents
from table_searcher import search_table_for_query
from llm.prompt_builder import build_prompt
from llm.inference import ask_gpt


def chatbot_pipeline(question: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ ìƒì„±

    Args:
        question (str): ì‚¬ìš©ì ìì—°ì–´ ì§ˆë¬¸

    Returns:
        str: GPT ì‘ë‹µ
    """
    print(f"[ì‚¬ìš©ì ì§ˆë¬¸] {question}")

    # Step 1. NER ì¶”ì¶œ
    query = extract_query_entities(question)
    if not query or not all(k in query for k in ["UNI", "TYPE", "KEYWORD"]):
        return "[ì˜¤ë¥˜] ì§ˆë¬¸ì—ì„œ í•„ìš”í•œ í‚¤ì›Œë“œ(ëŒ€í•™êµ/ì „í˜•/ì£¼ì œ)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    print(f"[ì¶”ì¶œëœ í‚¤ì›Œë“œ] {query}")

    # Step 2. ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = search_documents(query)
    if not retrieved_docs:
        return "[ì˜¤ë¥˜] ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # Step 3. í‘œ ê²€ìƒ‰
    table_data = search_table_for_query(query)

    # Step 4. í”„ë¡¬í”„íŠ¸ ìƒì„± (í…ìŠ¤íŠ¸ + í‘œ)
    prompt = build_prompt(query, retrieved_docs, table_data)

    # Step 5. GPT í˜¸ì¶œ
    response = ask_gpt(prompt)
    return response


if __name__ == "__main__":
    while True:
        user_input = input("\nì§ˆë¬¸ ì…ë ¥ ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if user_input.lower() == "exit":
            break

        answer = chatbot_pipeline(user_input)
        print("\nğŸ“Œ GPT ì‘ë‹µ:")
        print(answer)
